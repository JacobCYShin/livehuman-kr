'''
ğŸ¤ ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì…ë ¥ (PCM, 16KHz)
         â”‚
         â–¼
ğŸ§  LipASR.run_step()
    - ì˜¤ë””ì˜¤ í”„ë ˆì„ ëˆ„ì 
    - Mel-spectrogram ê³„ì‚°
    - 16-frame ë‹¨ìœ„ì˜ Mel chunk ë¦¬ìŠ¤íŠ¸ ìƒì„±
         â”‚
         â–¼
ğŸ“¤ feat_queue (ASR â†’ Inference ì—°ê²°)
         â”‚
         â–¼
ğŸ¨ inference()
    - ì–¼êµ´ ì´ë¯¸ì§€ & Mel chunkë¡œ Wav2Lip ì‹¤í–‰
    - ì… ëª¨ì–‘ì´ í¬í•¨ëœ ì˜ìƒ í”„ë ˆì„ ìƒì„±
         â”‚
         â–¼
ğŸ“º process_frames()
    - ì˜ìƒ í”„ë ˆì„ ìœ„ì— ì… ë®ê¸°
    - WebRTCë¡œ ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ í‘¸ì‹œ
'''

###############################################################################
# LiveTalking í”„ë¡œì íŠ¸ - ìŒì„± ê¸°ë°˜ ì… ëª¨ì–‘ ìƒì„± (ASR to Mel)
###############################################################################

import time
import torch
import numpy as np

import queue
from queue import Queue
# import multiprocessing as mp  # í˜„ì¬ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

from baseasr import BaseASR  # ê³µí†µ ASR ì²˜ë¦¬ í´ë˜ìŠ¤
from wav2lip import audio    # Mel-spectrogram ì¶”ì¶œ í•¨ìˆ˜ í¬í•¨

# BaseASRì„ ìƒì†ë°›ì•„ Wav2Lipì— ë§ëŠ” mel íŠ¹ì§• ì¶”ì¶œ ê¸°ëŠ¥ êµ¬í˜„
class LipASR(BaseASR):

    # í•œ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ìŒì„± â†’ Mel-spectrogram íŠ¹ì§• ì¶”ì¶œ
    def run_step(self):
        ##############################################
        # 1. ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì§‘ ë° ì¶œë ¥ í ì „ë‹¬
        ##############################################
        for _ in range(self.batch_size * 2):
            frame, type, eventpoint = self.get_audio_frame()  # 20ms ì˜¤ë””ì˜¤ ìˆ˜ì‹ 
            self.frames.append(frame)                         # ë‚´ë¶€ ë²„í¼ì— ì¶”ê°€
            self.output_queue.put((frame, type, eventpoint))  # inference()ë¡œ ì „ë‹¬

        ##############################################
        # 2. context ë¶€ì¡± ì‹œ íŠ¹ì§• ì¶”ì¶œ ìŠ¤í‚µ
        ##############################################
        if len(self.frames) <= self.stride_left_size + self.stride_right_size:
            return

        ##############################################
        # 3. Mel-spectrogram ìƒì„±
        ##############################################
        inputs = np.concatenate(self.frames)              # ì—°ì†ì ì¸ PCM ë°°ì—´
        mel = audio.melspectrogram(inputs)                # (80, T) mel ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±

        ##############################################
        # 4. strideë¥¼ ê³ ë ¤í•´ íŠ¹ì§• ìë¥´ê¸°
        ##############################################
        left = max(0, self.stride_left_size * 80 / 50)    # left stride ìœ„ì¹˜ ê³„ì‚°
        right = min(len(mel[0]), len(mel[0]) - self.stride_right_size * 80 / 50)

        mel_idx_multiplier = 80. * 2 / self.fps           # í”„ë ˆì„ë‹¹ Mel ê°„ê²© (ex: 3.2)
        mel_step_size = 16                                # Wav2Lipì—ì„œ ìš”êµ¬í•˜ëŠ” step size

        i = 0
        mel_chunks = []
        while i < (len(self.frames) - self.stride_left_size - self.stride_right_size) / 2:
            start_idx = int(left + i * mel_idx_multiplier)
            if start_idx + mel_step_size > len(mel[0]):
                # ëì— ë„ë‹¬í•˜ë©´ ë§ˆì§€ë§‰ mel ë²”ìœ„ ì˜ë¼ì„œ ë„£ê¸°
                mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])
            else:
                mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])
            i += 1

        ##############################################
        # 5. ê²°ê³¼ mel_chunk ë¦¬ìŠ¤íŠ¸ë¥¼ inference queueë¡œ ì „ë‹¬
        ##############################################
        self.feat_queue.put(mel_chunks)

        ##############################################
        # 6. ì˜¤ë˜ëœ í”„ë ˆì„ ì‚­ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        ##############################################
        self.frames = self.frames[-(self.stride_left_size + self.stride_right_size):]
