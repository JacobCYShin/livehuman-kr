'''
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       Audio In         â”‚  LipASR.run_step() â”€â”€â”€â”€â”€â”
        (stream)        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â–¼
                                               Mel + PCM
                                                  â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚ inference()â”‚ â—€â”€â”€â”€â”€ â”‚ face images â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                      í•©ì„±ëœ ìž… í”„ë ˆìž„
                             â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ì¢Œí‘œë¡œ ë®ê¸°   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ process_frames â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ WebRTC ì†¡ì¶œ â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

'''

###############################################################################
#  Copyright (C) 2024 LiveTalking@lipku https://github.com/lipku/LiveTalking
#  email: lipku@foxmail.com
# 
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#  
#       http://www.apache.org/licenses/LICENSE-2.0
# 
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
###############################################################################

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import math
import torch
import numpy as np

# ê¸°íƒ€ ìœ í‹¸
import os
import time
import cv2
import glob
import pickle
import copy

# í ë° ë©€í‹°ìŠ¤ë ˆë”©/í”„ë¡œì„¸ì‹± ê´€ë ¨
import queue
from queue import Queue
from threading import Thread, Event
import torch.multiprocessing as mp

# ìŒì„± ì¸ì‹ + Wav2Lip + í†µí•© ì²˜ë¦¬ ì‹œìŠ¤í…œ
from lipasr import LipASR
import asyncio
from av import AudioFrame, VideoFrame
from wav2lip.models import Wav2Lip
from basereal import BaseReal

from tqdm import tqdm
from logger import logger  # ë¡œê·¸ ì¶œë ¥ìš©

# ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ì„¤ì • (CUDA > MPS > CPU)
device = "cuda" if torch.cuda.is_available() else ("mps" if (hasattr(torch.backends, "mps") and torch.backends.mps.is_available()) else "cpu")
print('Using {} for inference.'.format(device))

# Wav2Lip ëª¨ë¸ checkpoint ë¡œë”©
def _load(checkpoint_path):
	if device == 'cuda':
		checkpoint = torch.load(checkpoint_path)
	else:
		checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)
	return checkpoint

# Wav2Lip ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ì ìš©
def load_model(path):
	model = Wav2Lip()
	logger.info("Load checkpoint from: {}".format(path))
	checkpoint = _load(path)
	s = checkpoint["state_dict"]
	new_s = {}
	for k, v in s.items():
		new_s[k.replace('module.', '')] = v  # multi-GPU ëª¨ë¸ í‚¤ ìˆ˜ì •
	model.load_state_dict(new_s)
	model = model.to(device)
	return model.eval()

# ì•„ë°”íƒ€ ë°ì´í„° (í”„ë ˆìž„/ì¢Œí‘œ) ë¡œë“œ
def load_avatar(avatar_id):
    avatar_path = f"./data/avatars/{avatar_id}"
    full_imgs_path = f"{avatar_path}/full_imgs"
    face_imgs_path = f"{avatar_path}/face_imgs"
    coords_path = f"{avatar_path}/coords.pkl"

    with open(coords_path, 'rb') as f:
        coord_list_cycle = pickle.load(f)
    input_img_list = glob.glob(os.path.join(full_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    input_img_list = sorted(input_img_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    frame_list_cycle = read_imgs(input_img_list)

    input_face_list = glob.glob(os.path.join(face_imgs_path, '*.[jpJP][pnPN]*[gG]'))
    input_face_list = sorted(input_face_list, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
    face_list_cycle = read_imgs(input_face_list)

    return frame_list_cycle, face_list_cycle, coord_list_cycle

# ëª¨ë¸ warm-upì„ ìœ„í•œ ë”ë¯¸ ìž…ë ¥ ì‹¤í–‰
@torch.no_grad()
def warm_up(batch_size, model, modelres):
    logger.info('warmup model...')
    img_batch = torch.ones(batch_size, 6, modelres, modelres).to(device)
    mel_batch = torch.ones(batch_size, 1, 80, 16).to(device)
    model(mel_batch, img_batch)

# ì´ë¯¸ì§€ ëª©ë¡ì„ cv2ë¡œ ì½ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
def read_imgs(img_list):
    frames = []
    logger.info('reading images...')
    for img_path in tqdm(img_list):
        frame = cv2.imread(img_path)
        frames.append(frame)
    return frames

# í”„ë ˆìž„ ë°˜ë³µ ì‹œ ì¢Œìš° ì™•ë³µ ë°©ì‹ìœ¼ë¡œ index ìˆœí™˜
def __mirror_index(size, index):
    turn = index // size
    res = index % size
    return res if turn % 2 == 0 else size - res - 1

# ë©”ì¸ ìž…ëª¨ì–‘ í•©ì„± ì²˜ë¦¬ ë£¨í”„
def inference(quit_event, batch_size, face_list_cycle, audio_feat_queue, audio_out_queue, res_frame_queue, model):
    length = len(face_list_cycle)
    index = 0
    count = 0
    counttime = 0
    logger.info('start inference')

    while not quit_event.is_set():
        starttime = time.perf_counter()
        mel_batch = []

        # ì˜¤ë””ì˜¤ íŠ¹ì§•(Mel spectrogram) ë°›ì•„ì˜¤ê¸°
        try:
            mel_batch = audio_feat_queue.get(block=True, timeout=1)
        except queue.Empty:
            continue

        is_all_silence = True
        audio_frames = []

        # ì˜¤ë””ì˜¤ í”„ë ˆìž„ ë°›ì•„ì˜¤ê¸° (2ê°œë‹¹ 1í”„ë ˆìž„)
        for _ in range(batch_size * 2):
            frame, type, eventpoint = audio_out_queue.get()
            audio_frames.append((frame, type, eventpoint))
            if type == 0:  # 0ì´ë©´ ì‹¤ì œ ë°œí™”
                is_all_silence = False

        if is_all_silence:
            # ì „ë¶€ ë¬´ìŒì´ë©´ ë¹ˆ í”„ë ˆìž„ ì²˜ë¦¬
            for i in range(batch_size):
                res_frame_queue.put((None, __mirror_index(length, index), audio_frames[i*2:i*2+2]))
                index += 1
        else:
            # ì–¼êµ´ ì´ë¯¸ì§€ì™€ ë©œìŠ¤íŽ™íŠ¸ë¡œê·¸ëž¨ì„ batchë¡œ êµ¬ì„±
            t = time.perf_counter()
            img_batch = []
            for i in range(batch_size):
                idx = __mirror_index(length, index+i)
                face = face_list_cycle[idx]
                img_batch.append(face)
            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)

            # ë§ˆìŠ¤í¬ ì²˜ë¦¬ (í•˜ë‹¨ ë°˜ì€ 0 ì²˜ë¦¬)
            img_masked = img_batch.copy()
            img_masked[:, face.shape[0]//2:] = 0
            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.0

            # mel: [B, 1, 80, 16] / image: [B, 6, H, W]
            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])
            img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)
            mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)

            with torch.no_grad():
                pred = model(mel_batch, img_batch)
            pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.0  # [B, H, W, C]

            counttime += (time.perf_counter() - t)
            count += batch_size
            if count >= 100:
                logger.info(f"------actual avg infer fps:{count/counttime:.4f}")
                count = 0
                counttime = 0

            # ê²°ê³¼ í”„ë ˆìž„ íì— ì‚½ìž…
            for i, res_frame in enumerate(pred):
                res_frame_queue.put((res_frame, __mirror_index(length, index), audio_frames[i*2:i*2+2]))
                index += 1

    logger.info('lipreal inference processor stop')

class LipReal(BaseReal):
    @torch.no_grad()
    def __init__(self, opt, model, avatar):
        super().__init__(opt)  # BaseReal ì´ˆê¸°í™” (tts, asr ë“± ê¸°ë³¸ ìš”ì†Œ í¬í•¨)
        self.W = opt.W  # ì˜ìƒ ê°€ë¡œ í¬ê¸°
        self.H = opt.H  # ì˜ìƒ ì„¸ë¡œ í¬ê¸°

        self.fps = opt.fps  # í”„ë ˆìž„ ì†ë„ (ex: 50 = 20ms ë‹¨ìœ„)

        self.batch_size = opt.batch_size
        self.idx = 0
        self.res_frame_queue = Queue(self.batch_size * 2)  # í•©ì„± ê²°ê³¼ í”„ë ˆìž„ ì €ìž¥ í

        self.model = model  # ë¯¸ë¦¬ ë¡œë“œëœ Wav2Lip ëª¨ë¸
        self.frame_list_cycle, self.face_list_cycle, self.coord_list_cycle = avatar  # ì•„ë°”íƒ€ ì´ë¯¸ì§€, ì–¼êµ´, ì¢Œí‘œ

        self.asr = LipASR(opt, self)  # ìž…ëª¨ì–‘ ë™ê¸°í™”ë¥¼ ìœ„í•œ ASR ì¸ì‹ê¸° ìƒì„±
        self.asr.warm_up()  # ASR warm-up

        self.render_event = mp.Event()  # (ë¯¸ì‚¬ìš©) ë Œë”ë§ ìƒíƒœ syncìš© ì´ë²¤íŠ¸ ê°ì²´

    def __del__(self):
        logger.info(f'lipreal({self.sessionid}) delete')  # ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì‹œ ë¡œê·¸ ì¶œë ¥

    # ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ í”„ë ˆìž„ì„ ê°€ì ¸ì™€ WebRTC ìŠ¤íŠ¸ë¦¼ì— ì†¡ì¶œí•˜ëŠ” í•¨ìˆ˜
    def process_frames(self, quit_event, loop=None, audio_track=None, video_track=None):
        while not quit_event.is_set():
            try:
                res_frame, idx, audio_frames = self.res_frame_queue.get(timeout=1)
            except queue.Empty:
                continue

            # ðŸ”‡ ë¬´ìŒ ìƒíƒœì´ë©´: full ì´ë¯¸ì§€ë§Œ ë³´ì—¬ì£¼ê³ , ìŒì„±ì€ None ì²˜ë¦¬
            if audio_frames[0][1] != 0 and audio_frames[1][1] != 0:
                self.speaking = False
                audiotype = audio_frames[0][1]
                if self.custom_index.get(audiotype) is not None:
                    mirindex = self.mirror_index(len(self.custom_img_cycle[audiotype]), self.custom_index[audiotype])
                    combine_frame = self.custom_img_cycle[audiotype][mirindex]
                    self.custom_index[audiotype] += 1
                else:
                    combine_frame = self.frame_list_cycle[idx]
            else:
                # ðŸ—£ ë°œí™” ìƒíƒœì´ë©´: í•©ì„±ëœ ìž… í”„ë ˆìž„ì„ ì–¼êµ´ ìœ„ì— ë®ì–´ì”€
                self.speaking = True
                bbox = self.coord_list_cycle[idx]
                combine_frame = copy.deepcopy(self.frame_list_cycle[idx])
                y1, y2, x1, x2 = bbox
                try:
                    res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                except:
                    continue
                combine_frame[y1:y2, x1:x2] = res_frame  # ìž…ë§Œ í•©ì„±

            # ì˜ìƒ í”„ë ˆìž„ì„ WebRTC ì „ì†¡
            image = combine_frame
            new_frame = VideoFrame.from_ndarray(image, format="bgr24")
            asyncio.run_coroutine_threadsafe(video_track._queue.put((new_frame, None)), loop)
            self.record_video_data(image)  # ì €ìž¥ (ì˜µì…˜)

            # ì˜¤ë””ì˜¤ë„ WebRTC ì „ì†¡
            for audio_frame in audio_frames:
                frame, type, eventpoint = audio_frame
                frame = (frame * 32767).astype(np.int16)
                new_frame = AudioFrame(format='s16', layout='mono', samples=frame.shape[0])
                new_frame.planes[0].update(frame.tobytes())
                new_frame.sample_rate = 16000
                asyncio.run_coroutine_threadsafe(audio_track._queue.put((new_frame, eventpoint)), loop)
                self.record_audio_data(frame)  # ì €ìž¥ (ì˜µì…˜)

        logger.info('lipreal process_frames thread stop')

    # ì „ì²´ ë Œë”ë§ ì‹¤í–‰ í•¨ìˆ˜: TTS + inference + frame ì²˜ë¦¬ thread ì‹¤í–‰
    def render(self, quit_event, loop=None, audio_track=None, video_track=None):
        self.tts.render(quit_event)  # TTS ì“°ë ˆë“œ ì‹œìž‘
        self.init_customindex()  # ì»¤ìŠ¤í…€ ì˜ìƒ index ì´ˆê¸°í™”

        # ë¹„ë””ì˜¤/ì˜¤ë””ì˜¤ í”„ë ˆìž„ ì²˜ë¦¬ ì“°ë ˆë“œ ì‹œìž‘
        process_thread = Thread(target=self.process_frames, args=(quit_event, loop, audio_track, video_track))
        process_thread.start()

        # Wav2Lip ì¶”ë¡ (inference) ì²˜ë¦¬ ì“°ë ˆë“œ ì‹œìž‘
        Thread(target=inference, args=(
            quit_event,
            self.batch_size,
            self.face_list_cycle,
            self.asr.feat_queue,
            self.asr.output_queue,
            self.res_frame_queue,
            self.model,
        )).start()

        # ë§¤ í”„ë ˆìž„ë§ˆë‹¤ run_stepìœ¼ë¡œ ASR ì—…ë°ì´íŠ¸ + í ìƒíƒœ ë³´ê³  sleep
        count = 0
        totaltime = 0
        _starttime = time.perf_counter()
        while not quit_event.is_set():
            t = time.perf_counter()
            self.asr.run_step()  # ì˜¤ë””ì˜¤ ìž…ë ¥ ì²˜ë¦¬

            # video_track íê°€ ë„ˆë¬´ ë§Žìœ¼ë©´ sleepìœ¼ë¡œ ì œì–´ (ë²„í¼ë§ ë°©ì§€)
            if video_track._queue.qsize() >= 5:
                logger.debug('sleep qsize=%d', video_track._queue.qsize())
                time.sleep(0.04 * video_track._queue.qsize() * 0.8)
                

        logger.info('lipreal thread stop')

